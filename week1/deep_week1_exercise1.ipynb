{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCD3ab-nHTXO"
      },
      "source": [
        "* take two different tokenizers mono and multi\n",
        "* write code to run some data through the tokenizers OUTPUT IN TEXTFORM\n",
        "* check how gpt4 tokenizes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** ALL ANSWERS AT THE BOTTOM OF THE NOTEBOOK***"
      ],
      "metadata": {
        "id": "Q-WVzA085rLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "skUo560r34HD"
      },
      "outputs": [],
      "source": [
        "# install transformers\n",
        "\n",
        "!pip install --quiet transformers wikipedia-api\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tfW13rT58KFU"
      },
      "outputs": [],
      "source": [
        "# import autotokenizer and wikipedia-api\n",
        "from transformers import AutoTokenizer\n",
        "import wikipediaapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US8VEiEY8mfZ",
        "outputId": "277733ec-9027-4b14-c42e-631d69be41b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# single language\n",
        "MONO=\"bert-base-cased\"\n",
        "mono_tokenizer=AutoTokenizer.from_pretrained(MONO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0LcgBzSA88_E"
      },
      "outputs": [],
      "source": [
        "# multilanguage\n",
        "MULTI=\"bert-base-multilingual-cased\"\n",
        "multi_tokenizer=AutoTokenizer.from_pretrained(MULTI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3ocgCQV4N_o"
      },
      "source": [
        "Write code to load the selected tokenizers and tokenize text using these (output is expected to be subword tokens in text form, not numbers). Select a piece of text (e.g. Wikipedia page or news article) written in your target language and tokenize it separately using both models. Inspect whether the tokenization results differ. How many subwords did each produce?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vo8SGGJuafWY"
      },
      "outputs": [],
      "source": [
        "# use wikipedia-api\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia('MyProjectName (merlin@example.com)', 'en')\n",
        "\n",
        "page_py = wiki_wiki.page('Python_(programming_language)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yEJliBVdazFZ"
      },
      "outputs": [],
      "source": [
        "summary = page_py.summary.replace(\"\\n\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "heVX6OJCbAVN",
        "outputId": "4fb846aa-a716-4d25-951e-7059e2523090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7-6DKQXbGz-"
      },
      "source": [
        "This works as my corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQCAiC7MbNRy",
        "outputId": "5175d2d3-2de6-4334-dee7-42902086abae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'Python': 9, 'is': 3, 'a': 4, 'high': 1, '-': 5, 'level': 1, ',': 8, 'general': 1, 'purpose': 1, 'programming': 5, 'language': 3, '.': 16, 'Its': 1, 'design': 1, 'philosophy': 1, 'emphasizes': 1, 'code': 1, 'readability': 1, 'with': 2, 'the': 6, 'use': 2, 'of': 3, 'significant': 1, 'indentation': 1, 'dynamically': 1, 'typed': 1, 'and': 4, 'garbage': 1, 'collected': 1, 'It': 2, 'supports': 1, 'multiple': 1, 'paradigms': 1, 'including': 1, 'structured': 1, '(': 1, 'particularly': 1, 'procedural': 1, ')': 1, 'object': 1, 'oriented': 1, 'functional': 1, 'often': 1, 'described': 1, 'as': 4, '\"': 2, 'batteries': 1, 'included': 1, 'due': 1, 'to': 2, 'its': 1, 'comprehensive': 1, 'standard': 1, 'library': 1, 'Guido': 1, 'van': 1, 'Rossum': 1, 'began': 1, 'working': 1, 'on': 1, 'in': 6, 'late': 1, '1980s': 1, 'successor': 1, 'ABC': 1, 'first': 1, 'released': 4, 'it': 1, '1991': 1, '0': 4, '9': 1, '2': 3, 'was': 3, '2000': 1, '3': 1, '2008': 1, 'major': 1, 'revision': 1, 'not': 1, 'completely': 1, 'backward': 1, 'compatible': 1, 'earlier': 1, 'versions': 1, '7': 1, '18': 1, '2020': 1, 'last': 1, 'release': 1, 'consistently': 1, 'ranks': 1, 'one': 1, 'most': 1, 'popular': 1, 'languages': 1, 'has': 1, 'gained': 1, 'widespread': 1, 'machine': 1, 'learning': 1, 'community': 1})\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "words_with_offsets = mono_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(summary)\n",
        "new_words = [w for w, o in words_with_offsets]\n",
        "for w in new_words:\n",
        "  word_freqs[w] += 1\n",
        "\n",
        "print(word_freqs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aBdIHAR0cFCx"
      },
      "outputs": [],
      "source": [
        "alphas = []\n",
        "\n",
        "for w in word_freqs.keys():\n",
        "  for l in w:\n",
        "    if l not in alphas:\n",
        "      alphas.append(l)\n",
        "\n",
        "alphas.sort()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d3w6kgh-dbmj"
      },
      "outputs": [],
      "source": [
        "splits = {word: [char for char in word] for word in word_freqs.keys()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tIG__xS6dm2z"
      },
      "outputs": [],
      "source": [
        "def compute_pair_freqs(splits) -> defaultdict:\n",
        "  pair_freqs = defaultdict(int)\n",
        "  for w, f in word_freqs.items():\n",
        "    split = splits[w]\n",
        "    if len(split) == 1:\n",
        "      continue\n",
        "    for i in range(len(split) - 1):\n",
        "      pair = (split[i], split[i + 1])\n",
        "      pair_freqs[pair] += f\n",
        "  return pair_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBCPRrNLeEFr",
        "outputId": "5ffc619c-3cba-4248-a87f-5dc67adc18cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('P', 'y'): 9\n",
            "('y', 't'): 9\n",
            "('t', 'h'): 17\n",
            "('h', 'o'): 9\n",
            "('o', 'n'): 16\n",
            "('i', 's'): 5\n"
          ]
        }
      ],
      "source": [
        "pair_freqs = compute_pair_freqs(splits)\n",
        "\n",
        "# part of the dict\n",
        "for i, k in enumerate(pair_freqs.keys()):\n",
        "  print(f'{k}: {pair_freqs[k]}')\n",
        "  if i >= 5:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx_wE9fZenkp",
        "outputId": "68d7d0e9-271d-45a9-f4a1-3d3753df7fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i', 'n') 19\n"
          ]
        }
      ],
      "source": [
        "\n",
        "most = \"\"\n",
        "freq = None\n",
        "\n",
        "for p, f in pair_freqs.items():\n",
        "  if freq is None or f > freq:\n",
        "    most = p\n",
        "    freq = f\n",
        "\n",
        "print(most, freq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WxTegYCg_VE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eOXnGtSSf86a"
      },
      "outputs": [],
      "source": [
        "vocab = alphas.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YMIk76W-g1D2"
      },
      "outputs": [],
      "source": [
        "\n",
        "merges = {most: \"\".join(most)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8LlxdVE_hCiZ"
      },
      "outputs": [],
      "source": [
        "vocab.append(merges[most])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9rAa1uccgHes"
      },
      "outputs": [],
      "source": [
        "def merge_pair(a,b, splits):\n",
        "  for w in word_freqs:\n",
        "    split = splits[w]\n",
        "    if len(split) == 1:\n",
        "      continue\n",
        "\n",
        "    i = 0\n",
        "    while i < len(split) - 1:\n",
        "      if split[i] == a and split[i+1] == b:\n",
        "        split = split[:i] + [a+b]+split[i+2:]\n",
        "      else:\n",
        "        i += 1\n",
        "    splits[w] = split\n",
        "  return splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Rawb-B5yjpJZ"
      },
      "outputs": [],
      "source": [
        "splits = merge_pair(most[0], most[1], splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kSbSNunpkOQb"
      },
      "outputs": [],
      "source": [
        "vocab_size = 330\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uNLV4a5lQXr",
        "outputId": "4c666d48-d946-40c8-819c-ea0b0e7d0408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('i', 'n'): 'in', ('t', 'h'): 'th', ('o', 'n'): 'on', ('a', 's'): 'as', ('a', 'n'): 'an', ('e', 'd'): 'ed', ('l', 'e'): 'le', ('a', 'r'): 'ar', ('P', 'y'): 'Py', ('Py', 'th'): 'Pyth', ('Pyth', 'on'): 'Python', ('r', 'e'): 're', ('in', 'g'): 'ing', ('r', 'a'): 'ra', ('e', 's'): 'es', ('e', 'n'): 'en', ('p', 'r'): 'pr', ('pr', 'o'): 'pro', ('m', 'm'): 'mm', ('c', 'o'): 'co', ('th', 'e'): 'the', ('i', 's'): 'is', ('pro', 'g'): 'prog', ('prog', 'ra'): 'progra', ('progra', 'mm'): 'programm', ('programm', 'ing'): 'programming', ('a', 'g'): 'ag', ('t', 'i'): 'ti', ('an', 'd'): 'and', ('o', 'r'): 'or', ('re', 'le'): 'rele', ('rele', 'as'): 'releas', ('i', 'g'): 'ig', ('o', 's'): 'os', ('l', 'an'): 'lan', ('lan', 'g'): 'lang', ('lang', 'u'): 'langu', ('langu', 'ag'): 'languag', ('m', 'p'): 'mp', ('i', 't'): 'it', ('o', 'f'): 'of', ('l', 'y'): 'ly', ('releas', 'ed'): 'released', ('2', '0'): '20', ('v', 'e'): 've', ('languag', 'e'): 'language', ('I', 't'): 'It', ('d', 'es'): 'des', ('a', 'd'): 'ad', ('w', 'i'): 'wi', ('en', 't'): 'ent', ('c', 't'): 'ct', ('s', 'u'): 'su', ('u', 'l'): 'ul', ('s', 't'): 'st', ('t', 'e'): 'te', ('co', 'mp'): 'comp', ('w', 'as'): 'was', ('ra', 'l'): 'ral', ('u', 'r'): 'ur', ('ig', 'n'): 'ign', ('p', 'h'): 'ph', ('i', 'l'): 'il', ('h', 'as'): 'has', ('re', 'ad'): 'read', ('it', 'y'): 'ity', ('wi', 'th'): 'with', ('u', 's'): 'us', ('us', 'e'): 'use', ('f', 'i'): 'fi', ('a', 'ti'): 'ati', ('a', 'l'): 'al', ('p', 'ar'): 'par', ('in', 'c'): 'inc', ('inc', 'l'): 'incl', ('incl', 'u'): 'inclu', ('inclu', 'd'): 'includ', ('ul', 'ar'): 'ular', ('u', 'n'): 'un', ('r', 'i'): 'ri', ('b', 'a'): 'ba', ('t', 'o'): 'to', ('s', 'i'): 'si', ('ar', 'd'): 'ard', ('l', 'i'): 'li', ('1', '9'): '19', ('20', '0'): '200', ('m', 'a'): 'ma', ('on', 's'): 'ons', ('h', 'ig'): 'hig', ('hig', 'h'): 'high', ('le', 've'): 'leve', ('leve', 'l'): 'level', ('g', 'en'): 'gen', ('gen', 'e'): 'gene', ('gene', 'ral'): 'general', ('p', 'ur'): 'pur', ('pur', 'p'): 'purp', ('purp', 'os'): 'purpos', ('purpos', 'e'): 'purpose', ('It', 's'): 'Its', ('des', 'ign'): 'design', ('ph', 'il'): 'phil', ('phil', 'os'): 'philos', ('philos', 'o'): 'philoso', ('philoso', 'ph'): 'philosoph', ('philosoph', 'y'): 'philosophy', ('e', 'mp'): 'emp', ('emp', 'has'): 'emphas', ('emphas', 'i'): 'emphasi', ('emphasi', 'z'): 'emphasiz', ('emphasiz', 'es'): 'emphasizes', ('co', 'd'): 'cod', ('cod', 'e'): 'code', ('read', 'a'): 'reada', ('reada', 'b'): 'readab', ('readab', 'il'): 'readabil', ('readabil', 'ity'): 'readability', ('s', 'ign'): 'sign', ('sign', 'i'): 'signi', ('signi', 'fi'): 'signifi', ('signifi', 'c'): 'signific', ('signific', 'an'): 'significan', ('significan', 't'): 'significant', ('in', 'd'): 'ind', ('ind', 'ent'): 'indent', ('indent', 'ati'): 'indentati', ('indentati', 'on'): 'indentation', ('d', 'y'): 'dy', ('dy', 'n'): 'dyn', ('dyn', 'a'): 'dyna', ('dyna', 'm'): 'dynam', ('dynam', 'i'): 'dynami', ('dynami', 'c'): 'dynamic', ('dynamic', 'al'): 'dynamical', ('dynamical', 'ly'): 'dynamically', ('t', 'y'): 'ty', ('ty', 'p'): 'typ', ('typ', 'ed'): 'typed', ('g', 'ar'): 'gar', ('gar', 'b'): 'garb', ('garb', 'ag'): 'garbag', ('garbag', 'e'): 'garbage', ('co', 'l'): 'col', ('col', 'le'): 'colle', ('colle', 'ct'): 'collect', ('collect', 'ed'): 'collected', ('su', 'p'): 'sup', ('sup', 'p'): 'supp', ('supp', 'or'): 'suppor', ('suppor', 't'): 'support', ('support', 's'): 'supports', ('m', 'ul'): 'mul', ('mul', 'ti'): 'multi', ('multi', 'p'): 'multip', ('multip', 'le'): 'multiple', ('par', 'ad'): 'parad', ('parad', 'ig'): 'paradig', ('paradig', 'm'): 'paradigm', ('paradigm', 's'): 'paradigms', ('includ', 'ing'): 'including', ('st', 'r'): 'str', ('str', 'u'): 'stru', ('stru', 'ct'): 'struct', ('struct', 'ur'): 'structur', ('structur', 'ed'): 'structured', ('par', 'ti'): 'parti', ('parti', 'c'): 'partic', ('partic', 'ular'): 'particular', ('particular', 'ly'): 'particularly', ('pro', 'c'): 'proc', ('proc', 'ed'): 'proced', ('proced', 'u'): 'procedu', ('procedu', 'ral'): 'procedural', ('o', 'b'): 'ob', ('ob', 'j'): 'obj', ('obj', 'e'): 'obje', ('obje', 'ct'): 'object', ('or', 'i'): 'ori', ('ori', 'ent'): 'orient', ('orient', 'ed'): 'oriented', ('f', 'un'): 'fun', ('fun', 'c'): 'func', ('func', 'ti'): 'functi', ('functi', 'on'): 'function', ('function', 'al'): 'functional', ('of', 't'): 'oft', ('oft', 'en'): 'often', ('des', 'c'): 'desc', ('desc', 'ri'): 'descri', ('descri', 'b'): 'describ', ('describ', 'ed'): 'described', ('ba', 't'): 'bat', ('bat', 'te'): 'batte', ('batte', 'ri'): 'batteri', ('batteri', 'es'): 'batteries', ('includ', 'ed'): 'included', ('d', 'u'): 'du', ('du', 'e'): 'due', ('it', 's'): 'its', ('comp', 're'): 'compre', ('compre', 'h'): 'compreh', ('compreh', 'en'): 'comprehen', ('comprehen', 'si'): 'comprehensi', ('comprehensi', 've'): 'comprehensive', ('st', 'and'): 'stand', ('stand', 'ard'): 'standard', ('li', 'b'): 'lib', ('lib', 'r'): 'libr', ('libr', 'ar'): 'librar', ('librar', 'y'): 'library', ('G', 'u'): 'Gu', ('Gu', 'i'): 'Gui', ('Gui', 'd'): 'Guid', ('Guid', 'o'): 'Guido', ('v', 'an'): 'van', ('R', 'os'): 'Ros', ('Ros', 'su'): 'Rossu', ('Rossu', 'm'): 'Rossum', ('b', 'e'): 'be', ('be', 'g'): 'beg', ('beg', 'an'): 'began', ('w', 'or'): 'wor', ('wor', 'k'): 'work', ('work', 'ing'): 'working', ('l', 'a'): 'la', ('la', 'te'): 'late', ('19', '8'): '198', ('198', '0'): '1980', ('1980', 's'): '1980s', ('su', 'c'): 'suc', ('suc', 'c'): 'succ', ('succ', 'es'): 'succes', ('succes', 's'): 'success', ('success', 'or'): 'successor', ('A', 'B'): 'AB', ('AB', 'C'): 'ABC', ('fi', 'r'): 'fir', ('fir', 'st'): 'first', ('19', '9'): '199', ('199', '1'): '1991', ('200', '0'): '2000', ('200', '8'): '2008', ('ma', 'j'): 'maj', ('maj', 'or'): 'major', ('re', 'v'): 'rev', ('rev', 'is'): 'revis', ('revis', 'i'): 'revisi', ('revisi', 'on'): 'revision', ('n', 'o'): 'no', ('no', 't'): 'not', ('comp', 'le'): 'comple', ('comple', 'te'): 'complete', ('complete', 'ly'): 'completely', ('ba', 'c'): 'bac', ('bac', 'k'): 'back', ('back', 'w'): 'backw', ('backw', 'ard'): 'backward', ('comp', 'ati'): 'compati', ('compati', 'b'): 'compatib', ('compatib', 'le'): 'compatible', ('e', 'ar'): 'ear', ('ear', 'li'): 'earli', ('earli', 'e'): 'earlie', ('earlie', 'r'): 'earlier', ('ve', 'r'): 'ver', ('ver', 'si'): 'versi', ('versi', 'ons'): 'versions', ('1', '8'): '18', ('20', '20'): '2020', ('l', 'as'): 'las', ('las', 't'): 'last', ('releas', 'e'): 'release', ('c', 'ons'): 'cons', ('cons', 'is'): 'consis', ('consis', 't'): 'consist', ('consist', 'ent'): 'consistent', ('consistent', 'ly'): 'consistently', ('r', 'an'): 'ran', ('ran', 'k'): 'rank', ('rank', 's'): 'ranks', ('on', 'e'): 'one', ('m', 'os'): 'mos', ('mos', 't'): 'most', ('p', 'o'): 'po', ('po', 'p'): 'pop'}\n"
          ]
        }
      ],
      "source": [
        "print(merges)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI"
      ],
      "metadata": {
        "id": "D_Bu28T4v3RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_w_freqs = defaultdict(int)\n",
        "\n",
        "multi_words_with_offsets = multi_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(summary)\n",
        "nw = [w for w,o in multi_words_with_offsets]\n",
        "for w in nw:\n",
        "  multi_w_freqs[w] += 1\n",
        "\n",
        "print(multi_w_freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1WOiId_v5TQ",
        "outputId": "66d24b26-8d33-4e16-cd6e-813479c532c4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'Python': 9, 'is': 3, 'a': 4, 'high': 1, '-': 5, 'level': 1, ',': 8, 'general': 1, 'purpose': 1, 'programming': 5, 'language': 3, '.': 16, 'Its': 1, 'design': 1, 'philosophy': 1, 'emphasizes': 1, 'code': 1, 'readability': 1, 'with': 2, 'the': 6, 'use': 2, 'of': 3, 'significant': 1, 'indentation': 1, 'dynamically': 1, 'typed': 1, 'and': 4, 'garbage': 1, 'collected': 1, 'It': 2, 'supports': 1, 'multiple': 1, 'paradigms': 1, 'including': 1, 'structured': 1, '(': 1, 'particularly': 1, 'procedural': 1, ')': 1, 'object': 1, 'oriented': 1, 'functional': 1, 'often': 1, 'described': 1, 'as': 4, '\"': 2, 'batteries': 1, 'included': 1, 'due': 1, 'to': 2, 'its': 1, 'comprehensive': 1, 'standard': 1, 'library': 1, 'Guido': 1, 'van': 1, 'Rossum': 1, 'began': 1, 'working': 1, 'on': 1, 'in': 6, 'late': 1, '1980s': 1, 'successor': 1, 'ABC': 1, 'first': 1, 'released': 4, 'it': 1, '1991': 1, '0': 4, '9': 1, '2': 3, 'was': 3, '2000': 1, '3': 1, '2008': 1, 'major': 1, 'revision': 1, 'not': 1, 'completely': 1, 'backward': 1, 'compatible': 1, 'earlier': 1, 'versions': 1, '7': 1, '18': 1, '2020': 1, 'last': 1, 'release': 1, 'consistently': 1, 'ranks': 1, 'one': 1, 'most': 1, 'popular': 1, 'languages': 1, 'has': 1, 'gained': 1, 'widespread': 1, 'machine': 1, 'learning': 1, 'community': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multialphas = []\n",
        "\n",
        "for w in multi_w_freqs.keys():\n",
        "  for l in w:\n",
        "    if l not in multialphas:\n",
        "      multialphas.append(l)\n",
        "\n",
        "multialphas.sort()\n",
        "print(multialphas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLVwt7qvwadl",
        "outputId": "34338c33-a073-4aa9-f82f-8ecf2890553a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', '(', ')', ',', '-', '.', '0', '1', '2', '3', '7', '8', '9', 'A', 'B', 'C', 'G', 'I', 'P', 'R', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multivocab = multialphas.copy()"
      ],
      "metadata": {
        "id": "6Yi5J6u7xANA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multisplits = {w: [c for c in w] for w in multi_w_freqs.keys()}"
      ],
      "metadata": {
        "id": "raFq6zr9xIJg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpfs = compute_pair_freqs(multisplits)\n",
        "\n",
        "for i, k in enumerate(mpfs.keys()):\n",
        "  print(f\"{k}: {mpfs[k]}\")\n",
        "  if i >= 5:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd4X8qJGxVtr",
        "outputId": "ad6c3374-f3b8-4f9d-8052-6919e94645e5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('P', 'y'): 9\n",
            "('y', 't'): 9\n",
            "('t', 'h'): 17\n",
            "('h', 'o'): 9\n",
            "('o', 'n'): 16\n",
            "('i', 's'): 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in mpfs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6162-8cy_YW",
        "outputId": "f002a5d9-eb8e-4b83-fb0e-225f66aeee9d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i', 'n') 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_merges = {best_pair: \"\".join(best_pair)}"
      ],
      "metadata": {
        "id": "F5y0e7N8z_xG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.append(multi_merges[best_pair])"
      ],
      "metadata": {
        "id": "Lss6ybTM0aKx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multisplits = merge_pair(best_pair[0], best_pair[1], multisplits)"
      ],
      "metadata": {
        "id": "UhkUK7bk1V51"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(multisplits[\"Python\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN22YM9J1bdw",
        "outputId": "cde9489d-5c35-493f-f6c8-3755c7fdcfce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['P', 'y', 't', 'h', 'o', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 300\n",
        "\n",
        "while len(multivocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(multisplits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    multisplits = merge_pair(*best_pair, multisplits)\n",
        "    multi_merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    multivocab.append(best_pair[0] + best_pair[1])"
      ],
      "metadata": {
        "id": "DIIqJknh1j4B"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEIc9EB937Bl"
      },
      "source": [
        "What are the vocabulary sizes in these models? Keeping in mind that the multilingual model is trained on several languages (how many?), how do these compare?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# single language\n",
        "print(\"vocabulary size for mono: \", mono_tokenizer.vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoDfjjcY5hxJ",
        "outputId": "4a8a0b35-4f2b-4746-94b9-1a42504a0730"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size for mono:  28996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multi language\n",
        "print(\"count of different languages: 104\") # from https://huggingface.co/google-bert/bert-base-multilingual-cased\n",
        "print(\"vocabulary size for multilanguage: \",multi_tokenizer.vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vieZvTYk5j6L",
        "outputId": "05878b86-4dab-469a-e5cd-656351cc35f2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of different languages: 104\n",
            "vocabulary size for multilanguage:  119547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Token count for monolanguage subword tokenizer: {len(merges)}')\n",
        "print(f'Token count for multilanguage subword tokenizer: {len(multi_merges)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRdo_X3v5Vx_",
        "outputId": "574c4e35-9be1-4287-d54d-80ba75dec9e8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count for monolanguage subword tokenizer: 286\n",
            "Token count for multilanguage subword tokenizer: 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This webpage can be used to visualize how ChatGPT tokenizes text. Try what it does with English and Finnish (or any other smaller language) text. What is your take on this?"
      ],
      "metadata": {
        "id": "unC_1Ugv6oWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For text:\n",
        "\n",
        "Write code to load the selected tokenizers and tokenize text using these (output is expected to be subword tokens in text form, not numbers). Select a piece of text (e.g. Wikipedia page or news article) written in your target language and tokenize it separately using both models. Inspect whether the tokenization results differ. How many subwords did each produce?\n",
        "\n",
        "it generated 74 tokens\n",
        "\n",
        "And translated to finnish:\n",
        "\n",
        "Kirjoita koodi, joka lataa valitut tokenizerit ja tokenisoi tekstiä niiden avulla (tulosteen odotetaan olevan tekstimuotoisia sanan alaisia tokeneita, ei numeroita). Valitse kohdekielelläsi kirjoitettu teksti (esim. Wikipedia-sivu tai uutisartikkeli) ja tokenisoi se erikseen molemmilla malleilla. Tarkasta, eroavatko tokenisointitulokset toisistaan. Kuinka monta alasanaa kumpikin tuotti?\n",
        "\n",
        "It generated 140 tokens\n",
        "\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "In the page OpenAI says that by rule of thumb one token in english is roughly 4 chars. It seems that for finnish the char count per token is less than that. both texts have roughly the same length."
      ],
      "metadata": {
        "id": "G1fpLZFh6zQX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}